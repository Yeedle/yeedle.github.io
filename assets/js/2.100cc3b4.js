(window.webpackJsonp=window.webpackJsonp||[]).push([[2],{57:function(t,a,s){"use strict";s.r(a);var n=s(0),o=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("div",{staticClass:"content"},[s("p",[t._v("This is an iteration of something I did last summer when I had the incredible experience of attending the "),s("a",{attrs:{href:"https://ds3.research.microsoft.com/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Microsoft Reserch Data Science Summer School")]),t._v(" (DS3). For my team’s "),s("a",{attrs:{href:"https://www.microsoft.com/en-us/research/video/data-science-summer-school-2016-fare-share-flow-and-efficiency-in-nycs-taxi-system/",target:"_blank",rel:"noopener noreferrer"}},[t._v("presentation")]),t._v(", we wanted to include a kind-of "),s("a",{attrs:{href:"https://www.youtube.com/watch?v=jbkSRLYSojo",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hans Rosling moment")]),t._v(", where one of us gets to talk animatedly over an animated visualization of the flow of NYC’s taxi system, explaining the flow as the animation plays. The result of our efforts looked like this ("),s("a",{attrs:{href:"https://github.com/msr-ds3/nyctaxi/blob/master/flow/flow_analysis.R",target:"_blank",rel:"noopener noreferrer"}},[t._v("code here")]),t._v("):")]),s("p",[s("img",{attrs:{src:"https://github.com/msr-ds3/nyctaxi/blob/master/figures/weekdays_cumsum_flow.gif?raw=true",alt:"old gif"}})]),s("p",[t._v("It was kind of a last minute thing, and I’ve always wanted to go back and redo it. Since last summer, the tidyverse has made a lot of progress, specifically in the area of spatial data with the appearence of "),s("code",[t._v("sf")]),t._v(", a tidy package for spatial data manipulation, and I decided to redo it using "),s("a",{attrs:{href:"https://github.com/edzer/sfr",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("sf")])]),t._v(", combined with "),s("a",{attrs:{href:"https://github.com/thomasp85/tweenr",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("tweenr")])]),t._v(", a package I discovered earlier this year which lets you create smoother animations, and other tidy tools.")]),s("p",[t._v("The data I used is freely available on the "),s("a",{attrs:{href:"http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml",target:"_blank",rel:"noopener noreferrer"}},[t._v("TLC’s website")]),t._v(". I chose to only work with the last 6 months of 2016, since the RAM on my laptop couldn’t handle more. Another cool thing that happened since last summer is that the TLC released a shapefile of the official taxi zones. So while the original animation relied on an “unofficial” source for NYC neighborhood boundaries, I now had the ability to use the TLC’s own shapefile.")]),s("h3",{attrs:{id:"how-to"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#how-to","aria-hidden":"true"}},[t._v("#")]),t._v(" How To")]),s("p",[t._v("The first step is, of course, downloading the data. You can do this by pointing and clicking at the above link, or by writing a shell script. Personally, I like when everything happens inside R, so here’s how I did that (using the recently released "),s("code",[t._v("glue")]),t._v(" package for string interpolation):")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tidyverse"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlibrary"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("glue"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndata "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" glue"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_{year}-{month}.csv"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             year "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"2016"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n             month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"07"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"08"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"09"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"10"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"11"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"12"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  map_df"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("possibly"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("read_csv"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" otherwise "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_frame"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  select"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickup_datetime "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tpep_pickup_datetime"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         dropoff_datetime "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" tpep_dropoff_datetime"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         passenger_count"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         pickup_zone_id "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" PULocationID"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         dropoff_zone_id "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" DOLocationID"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  drop_na"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[t._v("Here’s what’s happening line-by-line: Using "),s("code",[t._v("glue")]),t._v(", I created a list of file URLs. I then "),s("code",[t._v("map")]),t._v("ped the list to "),s("code",[t._v("possibly(read_csv)")]),t._v(" which does the heavy lifting of downloading the files, reading them into R as dataframes, and reducing everything to one big dataframe. Why only "),s("code",[t._v("possibly")]),t._v(" you ask? The Internet is weird. Don’t leave the fate of your downloads in its hands. "),s("code",[t._v("possibly")]),t._v(" gives you the option of specifying an "),s("code",[t._v("otherwise")]),t._v(" option in case of failure. In our case, an empty data frame works fine in case of failure.")]),s("p",[t._v("Since these files are quite big, this step takes a while. Since most of the dataframe is not needed and it’s just wasting the computer’s memory, I "),s("code",[t._v("select")]),t._v(" the 5 columns needed for the animation, and drop all the rows with "),s("code",[t._v("NA")]),t._v(" fields in them.")]),s("p",[t._v("The next step is getting the shapefile with the neighborhood information. Again, you can do this by pointing and clicking on the TLC’s website. I prefer having R doing it for me:")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("curl"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntemp "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" paste0"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tempdir"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"/taxi_zones.zip"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ncurl_download"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("temp"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntaxi_shapefile_path "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" paste0"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tempdir"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"/taxi_zones"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nunzip"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" exdir "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" taxi_shapefile_path"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nunlink"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("temp"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[t._v("That’s it. The shapefile are now downloaded and unzipped and sitting in the temporary directory.")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sf"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("## Linking to GEOS 3.5.0, GDAL 2.1.0, proj.4 4.9.2\n")])]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("magrittr"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nquietly"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("st_read"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("taxi_shapefile_path"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"taxi_zones"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  extract2"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"result"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  ggplot"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" \n  geom_sf"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("aes"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" borough"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  theme_minimal"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[s("img",{attrs:{src:"https://github.com/Yeedle/yeedle.github.io/blob/old/figure/source/2017-05-08-NYC-taxi-flow/read_shapefiles-1.png?raw=true",alt:"plot_of_taxi_zones"}})]),s("p",[t._v("Cool! Unfortunately, as you might discern from the map, the taxi zones defined for some neighborhoods are a bit too granular for what I want. I used the "),s("a",{attrs:{href:"https://github.com/dgrtwo/fuzzyjoin",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("fuzzyjoin")])]),t._v(" package to combine the taxi zones into bigger neighborhoods, leaving the polygon dissolving to "),s("code",[t._v("sf")]),t._v(".")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fuzzyjoin"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nneighborhoods "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data_frame"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  zone "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),s("span",{attrs:{class:"token string"}},[t._v('"Battery Park"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Clinton"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Crown Heights"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Chelsea"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Harlem"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),s("span",{attrs:{class:"token string"}},[t._v('"Financial District|World Trade Center"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Greenwich Village"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),s("span",{attrs:{class:"token string"}},[t._v('"Lenox Hill"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Lincoln Square"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Midtown"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Turtle Bay"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Williamsburg"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),s("span",{attrs:{class:"token string"}},[t._v('"Upper East Side"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Upper West Side"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"West Village"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{attrs:{class:"token string"}},[t._v('"SoHo|Hudson Sq"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{attrs:{class:"token string"}},[t._v('"Washington Heights"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Yorkville"')]),t._v("  \n  "),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nzones "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" quietly"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("st_read"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("taxi_shapefile_path"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"taxi_zones"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  extract2"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"result"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  st_set_precision"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("4")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  regex_left_join"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("neighborhoods"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"zone"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" if_else"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token operator"}},[t._v("!")]),t._v("is.na"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone.y"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zone.y"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" as.character"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone.x"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  select"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("LocationID"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" borough"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" geometry"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[t._v("This creates an "),s("code",[t._v("sf")]),t._v(" object, which we’ll use for plotting NYC’s map. But for now, we don’t need the "),s("code",[t._v("sf")]),t._v(" attributes, and since dataframe operations are faster, I ceated a dataframe with the same information as in "),s("code",[t._v("zones")]),t._v(" minus the "),s("code",[t._v("geometry")]),t._v(" list column.")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("zone_df "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" zones "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" as_tibble"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" select"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("geometry"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[t._v("This is all we need to add the location names to the "),s("code",[t._v("data")]),t._v(" dataframe:")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("data "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  left_join"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone_df"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"pickup_zone_id"')]),s("span",{attrs:{class:"token operator"}},[t._v("=")]),s("span",{attrs:{class:"token string"}},[t._v('"LocationID"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  rename"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickup_zone "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  left_join"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone_df"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"dropoff_zone_id"')]),s("span",{attrs:{class:"token operator"}},[t._v("=")]),s("span",{attrs:{class:"token string"}},[t._v('"LocationID"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  rename"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dropoff_zone "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[t._v("Ok, with all the prelimenary data setup out of the way, it’s time to do the actual work. One way to capture the concept of the flow of people in a city, is to measure the difference of people who entered a given neighborhood and people who left it at a given time. This gives us a “flow score” for that particular neighborhood at that point in time. This is similar to how flow in a network is measured, and it’s a rough measure of how people move about the city using yellow cabs. This is particularly useful for visulizaiton purposes, where we can map a color palette to the range of ‘flow’ values over all the neighborhoods.")]),s("p",[t._v("Since getting a moment-to-moment flow score is not very useful (because not much change happens at any given moment in most of NYC neighborhoods), it’s more useful to compute a neighborhood’s flow score at the beginiing of each hour of the day, and then interpolate the scores between the hour. In our data, which stretches over a 6 month period, we can calculate the score for every day at every hour, and then take the average.")]),s("p",[t._v("There was only one problem with this approach. Some neighborhoods, at certain times in the day, have zero taxi cabs visiting them. To ensure that the average also takes into account the times when nothing happened, I created an empty dataframe, which I then joined with the original dataframe, to fill in the missing hours and days.")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lubridate"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nempty_data "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v("  zone_df "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  distinct"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" map"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("~")]),t._v("tibble"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" rep"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("7")]),s("span",{attrs:{class:"token operator"}},[t._v(":")]),s("span",{attrs:{class:"token number"}},[t._v("12")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  unnest"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("days_in_month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" days_in_month"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("month"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         days_and_hours "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" map"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("days_in_month"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              "),s("span",{attrs:{class:"token operator"}},[t._v("~")]),t._v("tibble"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("day_in_month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" rep"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("1")]),s("span",{attrs:{class:"token operator"}},[t._v(":")]),t._v(".x"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" each "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("24")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                      hour_of_day "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" rep"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token operator"}},[t._v(":")]),s("span",{attrs:{class:"token number"}},[t._v("23")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" .x"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  unnest"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("date "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_date"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("year "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("2016")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" month "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" month"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" day "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" day_in_month"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  select"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("days_in_month"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("day_in_month"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("month"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nhead"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("empty_data"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("## # A tibble: 6 × 3\n##             zone hour_of_day       date\n##              chr         int       date\n## 1 Newark Airport           0 2016-07-01\n## 2 Newark Airport           1 2016-07-01\n## 3 Newark Airport           2 2016-07-01\n## 4 Newark Airport           3 2016-07-01\n## 5 Newark Airport           4 2016-07-01\n## 6 Newark Airport           5 2016-07-01\n")])]),s("p",[t._v("With the "),s("code",[t._v("empty_data")]),t._v(" dataframe ready, the flow scores can be calculated:")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("data "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" hour"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickup_datetime"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n         date "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" as_date"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pickup_datetime"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  gather"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pickup_zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dropoff_zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" if_else"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("type "),s("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"pickup_zone"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("passenger_count"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" passenger_count"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  group_by"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" date"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  summarize"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" sum"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  right_join"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("empty_data"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('"zone"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"date"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"hour_of_day"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  replace_na"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("list"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  group_by"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  summarize"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("log_avg "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" log10"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("abs"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mean"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("1")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("*")]),t._v(" sign"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mean"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("total"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\nhead"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("## # A tibble: 6 × 3\n## # Groups: zone [1]\n##                      zone hour_of_day   log_avg\n##               &lt;chr &gt;      &lt;int&gt;     &lt;dbl&gt;\n## 1 Allerton/Pelham Gardens           0 0.5317565\n## 2 Allerton/Pelham Gardens           1 0.4072800\n## 3 Allerton/Pelham Gardens           2 0.3815859\n## 4 Allerton/Pelham Gardens           3 0.3010300\n## 5 Allerton/Pelham Gardens           4 0.2602270\n## 6 Allerton/Pelham Gardens           5 0.1365827\n")])]),s("p",[t._v("This now gives us a dataframe with 24 rows per neighborhood, a row for each hour in the day, and for each neighborhood, for each hour, the log average “flow score.” This is enoguh to make the animation at the top of this post. But for the redo, I wanted something more smooth. This is where "),s("code",[t._v("tweenr")]),t._v(" comes in. "),s("code",[t._v("tweenr")]),t._v(" is a wonderful package that allows to interpolate data for smoother animation.")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tweenr"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndata_tweened "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" data "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  bind_rows"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filter"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hour_of_day "),s("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("24")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  arrange"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ease "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"linear"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" \n  tween_elements"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v("'hour_of_day'")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{attrs:{class:"token string"}},[t._v("'zone'")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{attrs:{class:"token string"}},[t._v("'ease'")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nframes "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("24")]),s("span",{attrs:{class:"token operator"}},[t._v("*")]),s("span",{attrs:{class:"token number"}},[t._v("10")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  left_join"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zones "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" group_by"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("zone"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v(" summarise"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" by "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token string"}},[t._v('".group"')]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"zone"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%>%")]),t._v("\n  mutate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("frame "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" as_factor"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paste0"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ifelse"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%%")]),s("span",{attrs:{class:"token number"}},[t._v("12")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v("'12'")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token percent-operator operator"}},[t._v("%%")]),s("span",{attrs:{class:"token number"}},[t._v("12")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                        "),s("span",{attrs:{class:"token string"}},[t._v('":"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                        ifelse"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token operator"}},[t._v("*")]),s("span",{attrs:{class:"token number"}},[t._v("60")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("10")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"0"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('""')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                        round"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token operator"}},[t._v("-")]),t._v("as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token operator"}},[t._v("*")]),s("span",{attrs:{class:"token number"}},[t._v("60")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                        ifelse"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("12")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("|")]),t._v(" as.integer"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hour_of_day"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("24")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('" AM"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{attrs:{class:"token string"}},[t._v('" PM"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("## Warning: Column `.group`/`zone` joining factor and character vector,\n## coercing into character vector\n")])]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("head"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_tweened"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("##   hour_of_day   log_avg .frame                  .group\n## 1           0 0.5317565      0 Allerton/Pelham Gardens\n## 2           0 1.8532484      0           Alphabet City\n## 3           0 0.1080942      0           Arden Heights\n## 4           0 0.2166248      0 Arrochar/Fort Wadsworth\n## 5           0 2.3311962      0                 Astoria\n## 6           0 0.2362414      0            Astoria Park\n##                         geometry    frame\n## 1 POLYGON((1026308.75 256767.... 12:00 AM\n## 2 POLYGON((992073.5 203714, 9... 12:00 AM\n## 3 POLYGON((935843.25 144283.2... 12:00 AM\n## 4 POLYGON((966568.75 158679.7... 12:00 AM\n## 5 POLYGON((1010804.25 218919.... 12:00 AM\n## 6 POLYGON((1005482.25 221686.... 12:00 AM\n")])]),s("p",[t._v("Here’s what happened in this chunk of code: in the first line I re-added hour 0 to the dataframe as hour 24 so that the animation is continously smoothed. Next the data is "),s("code",[t._v("arrange")]),t._v("d for each neighborhood in order of the hours. Then a column specifying the easing function to be used by "),s("code",[t._v("tweener")]),t._v(" is added to the dataframe. I wanted a realistic look, so I went for “linear.” Then, "),s("code",[t._v("tween_elements")]),t._v(" takes the dataframe, and interpolates the data as specified in the argumennts. I specified 240 frame, 10 for each hour, figuring that would give me a smooth enough animation yet not a too large file.")]),s("p",[t._v("Once the data is tweened, it is handed off to "),s("code",[t._v("left_join")]),t._v(" where it is rejoined with the "),s("code",[t._v("sf")]),t._v(" object “"),s("code",[t._v("zone")]),t._v("” created earlier. Note that "),s("code",[t._v("zone")]),t._v(" is grouped by zone and then summarize. This is to dissolve the multiple taxi zone polygons that compose one neighborhood, as specified in the neighborhood dataframe we "),s("code",[t._v("left_join")]),t._v("ed earlier. Finally, a new column called "),s("code",[t._v("frame")]),t._v(" is added to the dataframe, in which the meaningless interpolated "),s("code",[t._v("hour_of_day")]),t._v(" column is turned into meaninful time stamps. And now, showtime:")]),s("pre",{pre:!0,attrs:{class:"language-r"}},[s("code",[t._v("library"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("gganimate"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nplot "),s("span",{attrs:{class:"token operator"}},[t._v("<-")]),t._v(" ggplot"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data_tweened "),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" aes"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fill "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" log_avg"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" frame "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" frame"),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  geom_sf"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  scale_fill_distiller"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("palette "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"RdBu"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" na.value "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"#808080"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" guide "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"legend"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       name "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"average # of people"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       breaks "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("3")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("2")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("1")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("1")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("2")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("3")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                       labels "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" c"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token number"}},[t._v("1000")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("100")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("10")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("0")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("10")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("100")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("-")]),s("span",{attrs:{class:"token number"}},[t._v("1000")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  theme_ipsum"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  theme"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("axis.text "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" element_blank"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        axis.ticks "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" element_blank"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        axis.title "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" element_blank"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        panel.background "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" element_blank"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{attrs:{class:"token operator"}},[t._v("+")]),t._v("\n  labs"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("title "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Time: "')]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" caption "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"Average daily flow of people using the NYC Taxi system"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\ngganimate"),s("span",{attrs:{class:"token punctuation"}},[t._v("(")]),t._v("plot"),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ani.width "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("960")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ani.height "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v("960")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" interval "),s("span",{attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{attrs:{class:"token number"}},[t._v(".05")]),s("span",{attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{attrs:{class:"token string"}},[t._v('"taxi.gif"')]),s("span",{attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),s("p",[s("img",{attrs:{src:"https://github.com/yeedle/yeedle.github.io/blob/old/figure/source/2016-09-11-NYC-taxi-flow/taxi.gif?raw=true",alt:"new animation"}})]),s("p",[t._v("Voilà!")])])}],!1,null,null,null);a.default=o.exports}}]);